# Intro
## What is an Operating System
An operating system is the most important software that runs on a computer and provides services to applications. It manages the computer's memory, processes, and all of its software and hardware. It also allows a user to communicate with the computer without knowing how to speak the computer's language and its details.

- The computer can be a networked system with many servers.
- It hides the "messy" details using a level of **abstraction**.
- It presents the user with a simplified machine, much **easyer to use**.
- It executes user's programs on a specific platform.
- It uses the computer hardware and resources **efficiently**.

## Operating System Structure

Client-Server model

- A process is an instance of a program running in a computer; they may run concurrently.

# Services, System Calls & Structures

## OS Services
### System Boot

- Is short for bootstrap, which is the process to load the first piece of software into memory that starts a computer.
- OS is essential for running all other programs, it is usually the first piece of software loaded during the boot processs.
- Boot also involves loading other basic software
- **Cold Boot**: is when you turn the computer on from an off position.
- **Warm Boot**: is when you reset a computer that is already on.
- Typically, execution starts at a fixed memory location.

### User Interface

#### CLI/Command Interpreter

#### GUI

#### Touchscreen

### Program Execution


 
The system must be able to:
 

- load a program into memory
- run that program
- end execution(normally or abnormally)

 

### I/O Operation


 
A running program may require I/O, which may involve:
 

- A file and/or I/O devices.
- No direct user control - the OS handles devices

 

### File System Manipulation


 
Programs need to:
 

- read and write, create and delete - files and directories.
- search and list the file information.
- perform permission management.

 

### Communications


 
Processes may exchange information
 

- on the same computer.
- between computers over a network. Either via: shared memory or message passing(packets moved by the OS)

 

### Error Detection


 

- OS needs to be constantly aware of possible errors
- **May occur in**: the CPU; memory; I/O devices; a users program...
- Each type of error has its own **error handler**
- OS should take the appropriate action to ensure correct and consistent computing
- **Debugging** facilities for the user programs enhance system usability.

 

### Resource Allocation & Concurrency


 
Muti-Users or Multi-job
Resources must:
 

- be allocated to each of them
- Many types of resources

 
#### Extra
 
**Accounting**: To keep track of which users use how much and what kinds of computer resources.
**Protection and Security**: The owner s of information stroed in a mutiuser or networked computer system may want to control use of what information, concurrent processes should not interfere with each other.
 

- **Protection**: 
   - controls all access to system resources
   - prevents interference

 

- **Security**: 
   - Authentication - WHo
   - Authorization - What
   - Accounting

 

## System Calls

- OS provides programming interface to services.
- Typically written in a high-level language (C or C++)
- It is how a program requests a service from an OS’s kernel
- May include hardware-related services (for example, accessing a hard disk drive), creation and execution of new processes, and communication with integral kernel services such as process scheduling

**Application Programming Interface (API) – higher level of abstraction**

- High-level - usually used by programs
- Rather than direct use of system calls
- Usually less detailed than raw calls

### System Call Parameter Passing


 

1. **Pass the parameters in registers**
1.1. Simplest
1.2. But in some cases, there may be more parameters than registers
2. **Parameters stored in a block/table in memory**
2.1. Address of block passed as a parameter in a register
2.2. e.g. Linux and Solaris
3. **Parameters placed in the Stack**
3.1. Push: onto the stack by the program Pop: off the stack by the operating system
3.2. No limit to the number/length of parameters being passed

 

### Types of System Calls


 

- Process Control
- File Management
- Device Management
- Information Maintenance
- Communications
- Protection

 

### System Programs
System programs provide a convenient environment for program development and execution.

 
They can be divided into:
 

- File manipulation
- Status information sometimes stored in a file modification
- Programming language support
- Program loading and execution
- Communications
- Background services
- Application programs

 

**Most users' view of the opreation system is defined by system programs, not the actual system calls.**

## OS Structures

A general-purpose OS is a very large program
Category:

- Simple structure: MS-DOS
- Monolithic kernel: UNIX
- Micro kernel: MINIX(i.e. Mini-Unix)
- Layered: an academic abstraction
- Others

### OS Kernel

The OS kernel is a computer program that constitutesthe central core of a computer's operating system.
It has complete control over everything that occurs in the system. As such, it is the first program loaded on startup, and then manages the remainder of the startup, as well as I/O requests from software, translating them into data processing instructions for the CPU.
It is also responsible for managing memory, and formanaging and communicating with external devices, like printers, speakers, etc. The kernel is a fundamental part of a modern computer's operating system.
Usually loaded into a **protected area of memory**
Performs its tasks in **kernel space**, User does in **user space**
This separation prevents user data and kernel data from interfering with each other and thereby reducing performance or causing the system to become unstable (and possibly crashing).

#### Simple Stucture - MS-DOS


 
MS-DOS – written to provide the most functionality in the least space
 

- Not divided into modules
- Has some simple structure
- interfaces and levels of functionalities are not well separated

 

#### Complex Structure - UNIX


 
Consists of tow separable parts:
 

- Systems programs
- The kernel 
   - Everything below the system call interface and above the physical hardware.
   - provides: file system, CPU scheduling, memory management ...

 

#### Microkernel - MINIX


 
Moves many OS features from the kernel into user space.
Communication takes place between user modules using **message passing.**
**Performance overhead** of user space to kernel space communication.
A micro kernel provides basic services such as a scheduler and inter-process communication, and it is responsible for implementing process isolation and security but may delegate authentication and authorisation to a service.
 
Benifits:
 

- Uniform interface on request made by a process.
- Extensibility: allows the addition of new services.
- Portability/Maintainability: changes needed to port the system to a new processor are made within the microkernel - not in the other services.
- Reliability: Modular design; Can be rigorously tested and
therefore trusted. Service failure often does not result in kernel crash.
- Distributed system support: Messages are sent without having to know the type of target machine.
- Object-oriented operating system: OS components are objects with clearly defined interfaces.

 
Disadvantages:
 

- **Less efficient:** As Operating System services execute in user space the kernel must perform multiple context switches in order to service the request.

 

#### Monolithic


 
A monolithic kernel contains a scheduler, access to hardware (via drivers) and file systems. Monolithic kernels often implement a plugable architecture to allow adding new features (kernel modules). The kernel is also responsible for process isolation and security
The **security** of a micro kernel compared to a monolithic kernel is not necessarily more secure.
In the monolithic kernel architecture, the core services are implemented in the kernel, no context switch is required (only an escalation of privilege). **Faster**.
 

#### Layered Approach

**ONLY in Lab**

 

- OS is divided into a number of layers (levels)
- Bottom layer (layer 0), is the hardware
- Highest (layer N) is the user interface
- Modularity: Layers only talk to the layer above and the layer below

 

#### Modules

 
Loadable Kernel Modules

- Found in many modern operating systems
- Uses object-oriented approach
- Each core component is separate
- Each talks to the others over known interfaces
- Each is loadable as needed within the kernel
- Overall, similar to layers but with more flexible

 

#### Hybrid Systems

Most modern operating systems are actually not one pure model

- Combines multiple approaches to balance performance, security, usability needs etc
- Linux and Solaris: kernels in kernel address space, so monolithic, plus modular for dynamic loading of functionality
- Windows: mostly monolithic, plus microkernel for different
subsystem **personalities**

#### Apple IOS

#### Android

# I/O and Interrupts
## I/O Management
Major components of OS design and operation

- I/O devices vary greatly
- Various methods to control them
- Performance management (e.g. response time)
- New types of devices to add
### Input/Output

- The two main jobs of a computer are** I/O** and **processing** –  and in many cases the processing is incidental.
- The role of the operating system in computer I/O is to **manage and control** I/O operations and I/O devices.
### Data speeds
memory hierarchy
## How devices communicate

- A device communicates with a computer system by **sending signals** over a cable or **even the air**(e.g. wireless)
- The device communicates with the computer via a connection point termed a **port** (e.g. a serial port)
- If one or more devices use a common set of wires, then the connection is called a **bus**.
### Device drivers
Functions specific to individual devices are encapsulated in kernel modules called **device drivers** (software)
A **hardware controller** (e.g. a printer controller) is a collection of electronics that can operate a port, a bus, or a device.
### How do controllers work
Essentially, a controller has one or more registers for data and control signals.
The CPU communicates with the controller by reading and writing bit patterns in these registers.
One way of doing this is through **memory-mapped I/O.** Essentially, device-control registers are mapped into the address space of the CPU (so that the CPU can have the direct access to them).
### Registers
An I/O port typically consists of four registers: **status**, **control**, **data-in**, and **data-out**.

- **Status** contains bits that can be read by the CPU – may indicate whether current command has been completed, whether a byte is available to be read from data-in, etc.
- **Control** can be written by the CPU to start a command or change the mode of a device (enable parity checking etc).
- **Data-in** is read by the CPU to get input.
- **Data-out** is written by the CPU to send output.
### Device polling
The basic handshaking notion.
The controller indicates its state through the busy bit in the status register.
The controller sets the busy bit when it is busy working, and clears the busy bit when it is resdy to accept the next command.
The CPU signals its wishes via the command-ready bit in the control register.
流程：
1. The host (e.g. CPU or controlling device) repeatedly reads the busy bit of the controller until it becomes clear.
2. When clear, the host writes in the control register and writes a byte into the data-out register.
3. The host sets the command-ready bit (set to 1).
4. When the controller senses command-ready bit is set, it sets busy bit (1).
5. The controller reads the control register and since write bit is set, it performs necessary I/O operations on the device. If the read bit is set to one instead of write bit, data from device is loaded into data-in register, which is further read by the host.
6. The controller clears the command-ready bit once everything is over, it clears error bit to show successful operation and resets busy bit (0).

Disadvantage: The CPU in step 1 is **busy-waiting**, or **polling**.
### Interrupts
The hardware mechanism that enables a device to **notify** the CPU is called an **interrupt**. 
The CPU hardware has a wire called the interrupt request line (**IRL or IRQ**) that the CPU senses  (checks) after executing every instruction.
When the CPU detects that a controller has asserted a **signal** on the IRL, the CPU saves a small amount of state and jumps to the **interrupt handler** routine at a fixed point in memory space.
We say a device controller **raises** an interrupt by asserting a signal on the IRL, the CPU **catches** the interrupt and **dispatches** it to the interrupt handler, and the handler **clears** the interrupt by servicing the device. 
More sophisticated interrupt-handling features:

- The ability to **defer interrupt handling** during critical processing.
- An efficient way to dispatch to the proper interrupt handler for a device without having to poll each device to see who raised it.
- We need **multi-level interrupts**, so that the OS can distinguish between high- and low-priority interrupts.

Provided by the **CPU** and by the **interrupt-controller hardware**.
Examples:

- Exceptions
- Page Faults
- System Calls
- Program Exceptions
- Timer
- I/O
### DMA
Programmed I/O (one byte at a time) for large datamovement is too CPU-intensive.

- Bypasses CPU transferring data directly.
- **Requires DMA controller**
- OS writes DMA command block into memory.
   - Source and destination addresses
   - Read or write mode
   - Count of bytes
   - Writes location of command block to DMA controller
   - Interrupts signal signals completion
### Different Methods for Device-CPU Communication

- Polling or Hand-shaking - CPU does nothing.
- Interrupts - CPU could be interrupted **too frequently**.
- DMA-Direct Memory Access: Use an added controller to manage data movement without the CPU.
## Software I/O support
### Software I/O
Layers of support at the software level:

- User-level I/O software.
- Device-independent operating system software
- Device drivers
- Interrupt handlers
- Hardware
### Device independent I/O software

- Uniform interfacing for device drivers
- Buffering
- Error reporting
- Allocating and releasing dedicate devices
- Providing a device-independent block size
### Performance Constraints
I/O is a major factor in system performance:

- Demands CPU to execute device driver, kernel I/O code
- Context switches due to interrupts
- Data copying
- Network traffic especially stressful
# Memory Management
## What and Why
### What
It provides ways to **dynamically allocate** portions of memory to programs at their request and **free** it for reuse when no longer needed.
**Critical** to any advanced computer system
Two levels: **OS level** and **Application level**(garbage collection or manual)
### Why
A **program** must be brought **from disk** into memory and placed within a **process** for it to be executed.
The CPU has direct access to main memory and registers **ONLY**.
A memory management unit or **MMU** only sees:

- **A stream of addresses + read requests**
- **An address + data and write requests**
### Memory Hierarchy

- Small amount of fast, expensive memory
- Some medium speed and price main memory
- Large amounts of slow cheap disk storeage

MMU handles memory hierarchy.
### Swapping
Exchange **process/data** between main memory and hard disk
Total physical memory space required by processes may **exceed** physical memory
Use algorithms for swapping(e.g. priority-based scheduling)
Major time is **data transfer time(context switch time), often very high**
### User application
Is a se of instructions to be followed one by one
Programs that are ready to execute form an **input queue**
## Simple Memory Management
### Requirements
A process may be (often) re-located in main memory due to swapping, which enables the OS to have a **larger pool** of ready-to-execute processes
User applications (for both instructions and data) may have their own **memory references** (or logical addresses) 
Processes should not be able to reference memory locations in another process without permission:  no interference allowed
**Safe sharing**: Must also allow several processes to access the same portion of main memory for the reasons of cooperation and efficiency, without compromising protection
### Physical and Logical Addresses
A **physical address** is a physical location in main memory
A **logical address** is a reference to a memory location at compiling time, independent of the physical structure/organisation of memory
Compilers produce code in which all memory references are logical addresses
### Logical Address Space
Is the set of all logical addresses generated by a program

- Ensures muti-process systems
- Defined by **Base & Limit** registers
- CPU check and ensures the memory access is between the limits.

![](https://cdn.nlark.com/yuque/0/2024/jpeg/35993002/1704686331380-f0ae0387-1783-47e3-a535-4bcadb9e451c.jpeg)
Privileged instructions can change register values
Prevents accidental or deliberate memory corruption
### MMU
Is a computer hardware unit having all memory references passed through itself, primarily performing the **translation** of logical memory addresses to physical addresses. It is usually implemented as part of the CPU
Example: Relocation Register(new name for Base Register)
### Memory Conservation
#### Application Design: Dynamic loading

- Routine is not loaded until it is called
   - Better memory-space utilization 
   - unused routine is never loaded
   - All routines kept on disk in re-locatable load format
- Useful Case
   - large amounts of code are needed to handle infrequently occurring cases 
- No special OS support is required
   - Implemented through program design (but not very efficient)
   - OS can help by providing libraries to implement dynamic loading
#### OS Design: Swapping
### Usage of Swapping
Standard swapping not used in modern operating systems But modified version common: Swap only when free memory extremely low
Variation found on many systems
Mobile Systems

- Not typically supported
- Othermethods to free memory if low
   - iOS asks apps to voluntarily relinquish allocated memory Read-only data thrown out and reloaded from flash if needed Failure to free can result in termination
   - Android terminates apps if free memory is low, but first writes application state to flash for fast restart
## Memory Allocation
Main Memory

- **Small and limitied**
- support both OS and user process
### Contiguous or Neighbouring Allocation

- Early method
- split into two partitions: **Operation system**, **User processes**. Each process contained in single contiguous section of memory.
### Multiple-Partition Allocation
Variable-partition sizes:

- Increases efficiency but creates holes in memory

Hole - block of available memory
Process arrival - memory allocated from a hole that is large enough
Process exiting - frees its partition, adjacent free partitions combined
#### Bitmap
| 1 | 2 | 3 | 4 | 5 | 6 |
| --- | --- | --- | --- | --- | --- |

#### linked-list
### Allocation Algorithm
#### First fit

- simplest
- Search list for the first hole that is big enough to service request
- Split hole into two parts: one to become allocated block, one as a smaller hole

Advantages: **Easy**, **Fast**
Disadvantages: Can lead to **memory fragmentation**.
#### Next fit

- Keep track of position of last allocated block
- Start next search from there

Advantages: Attempt to reduce memory fragmentation
Disadvantages: Provides slightly worse performance than First Fit
#### Best fit
Searches list for hole that best matches the actualsize.
Disadvantages:

- Slow because the whole list must be searched each time
- Also produces fragmentation with lots of small holes

**First fit** tends to produce larger holes on average
#### Worst fit
Search the whole list for the biggest hole
Idea is to reduce fragmentation by leaving large holes
Disadvantages: Need to search the whole list (although can be optimized)
### Internal/External Fragmentation
**External fragmentation** occurs when holes appear in otherwise contiguous memory blocks
**Internal fragmentation** occurs when the block of memory allocated to a process is larger than the memory that process requires
**Compaction**: Move all processes to one end of memory and holes to the other end (run-time and expensive operations)
# File System
## Files
Files are collections of related information defined by their creator. They commonly represent **programs** and **data**.
vital concepts
Allows:

- Store large amounts of data
- Allow information to survive the termination of a process
- concurrently access data

Operating systems implement the **abstract concept** of a file by managing mass-storage devices, such as tapes and disks.
### Types of file
### File Naming
### File Attributes
### File Organisation
### Path Names
### File Access
Sequential access:

- Read all bytes/records from the beginning
- Cannot jump around, but can rewind/back-up
- Was convenient when the medium was **magnetic tape**

Random access:

- Bytes/records can be read in any order
- Essential for **database** systems
- Can seek/read or read/seek
### File Operations
create, delete, open, close...
### File Fragmentation
和内存的外部碎片类似
## File Systems
### Layered File System

- Reduces complexity & redundancy
- Layer can be independently implemented
- Adds overhead
- Can decrease performance
#### Devices

- Disk provides in-place rewrite and random access
- I/O transfers performed in blocks of sectors (usually 512 bytes)
#### I/O Control (Interrupts and Device Drivers)

- Manage I/O devices at the I/O control layer
- Outputs low-level hardware specific commands to hardware controller
#### Basic File System

- Translates command to device driver
- Manages memory buffers and caches (allocation, freeing, replacement)
   - _Buffers hold data in transit_
   - _Caches hold frequently used data_
#### File organization module

- Understands files, logical address and physical blocks
- Translates logical block number to physical block number
- Manages free space, disk allocation
#### Logical file system

- Manages metadata information
- All file systems structure except the data itself: **Directory management, Protection**
- Translates file name into file number, file handle, location by maintaining file control blocks (inodes Unix)
- Examples: ISO9660, UFS, FFS, FAT, NTFS, ext2, ext3...
#### Application Programs
Use the file system
### Vertrual File Systems(VFS)
A VFS is software that forms an interface between an OS kernel and a more concrete file system
Same system call interface (API) for different types of file systems:

- file systems types
- network file system

Then dispatches operation to appropriate file system implementation routines
# Processes and Threads
## Processes
Sometimes called a **task** or **job**.
Includes: **program**, **data** and **execution state**
Distinct from Program:

- Program:  List of instructions (code) – **static** form
- Process: Activity of executing instructions – **dynamic**, run-time behaviour

A process is a run-time instance of a **computer program that is being executed**. It contains the program code and its current activity. Depending on the operating system, a process may be made up of multiple **threads of execution** that execute instructions concurrently.
Provides concurrency and protection
### Multiprogramming
Parallel process execution:

- Only pseudo-parallelism
- Multiple processors allow some real parallelism
- **Interleaved execution**
- **Switching **determined by
   - Scheduler
   - State
   - Priority
### Process Execution States
![](https://cdn.nlark.com/yuque/0/2024/jpeg/35993002/1704700911698-bbab652b-c569-4939-ac73-9e68e6daafdb.jpeg)
Waiting: is **not ready**
### Dispatcher
A dispatcher is a special program in an OS which comes into play after a (shore-term, CPU) scheduler. When the scheduler completes its job of selecting a process from the **ready queue**, it is the dispatcher that takes that process to the desired state/queue.
Function:

- **Switching context**
- Switching from kernel mode to user mode
- Jumping to the proper location in the user program to restart that program
### Process Control Block (PCB)

- Data structure used by OS to manage the execution of a process: PCB0 , PCB1, … 
- Describes process for scheduler, includes accounting and priority
- Indicates current process state
- Records state of registers, heap, stack, etc.

Context Switch progress:

- Stopping one process and starting another
- Register values and other state information of current process **stored** in PCBx
- Register values and other state information of new process** loaded** from PCBy
- Must be as efficient as possible. Context switching is pure overhead, no real work is done
### Process Creation(UNIX)
#### fork(): system call to create a new process

- Requires no arguments, returns identifier of the child process
- Creates an **exact copy** of the parents address space for new child process
- Child process continues executing from where the fork() call was made in the parent process
- Returns 0 to the newly created child process
#### exec(): system call that usually following the fork

- Loads a program into the process address space, over-writing the program already there
- Family of functions, particular function usage depends on how parameters should be passed and environment to use
```c
int main(){
    pid_t pid;
    pid = fork();
    if(pid<0){
        fprintf(stderr, "Fork Failed");
        exit(-1);
    }else if(pid==0){
    	execlp("/bin/ls", "ls", NULL);
    }else{
        wait(NULL);
        fprintf("Child Compete");
        exit(0);
    }
}
```
### Process Creation(Windows)
#### CreateProcess() family of functions

- Loads specified program into the address space of the child process
- Returns child **process handles** (kernel index of child processes) to parent
### Process Termination
Termination criteria example:

- Normal completion: Process indicates it has finished executing final statement
- **Bounds violation** (Seg Fault): Process attempts access to memory which it is not allowed to
- **Memory exceeded**: Process requires more memory than system has available
- **Arithmetic error**: Divide by zero, etc.
- **OS or operator termination**: OS or user terminates process
## Threads
What are threads:

- Multiple **concurrent paths** of execution within a **process**
- **Lightweight process** – a simplified form of process
- Unit of **dispatching** (execution)
- Threads in a process share address space
- Separate concurrency from protection
### Why use multithreading
Primary Benefits:

- Responsiveness: applications can continue executing while parts are blocked
- Resource sharing: Threads share code and data
- **Economy**: Cheaper to create threads than processes
- Multiprocessor utilisation: threads can utilise parallel execution

Separates resource ownership and scheduling
### Thread life-cycle
Process normally starts with single thread: Primary thread spawns new child threads to do work
Four most common operations on a thread:

- **Creation**: Create a new thread, specify the procedure that it will run
- **Exit**: The thread has finished it's work, calls library procedure to exit
- **Join**: Current thread waits for a specific thread to finish execution
- **Yield**: Voluntarily give up CPU time to let another thread run
### Thread Implementations
Execution state moved to Thread Control Block (TCB)
Implemented at the kernel level or the user level
Application threads must map to a kernel execution thread

- User level: Many to one mapping
- Kernel level: One to One mapping
- Hybrid (Solaris): Many to many mapping
#### User Level Threads
Threads exist only in user space, kernel knows nothing about them

- Require a runtime package to implement the threads and keep track of them
- Can be used on systems that do not support threading

Benefits:

- Cheap to create
- Cheap context switches
- Thread yields don't result in process context switch
- Application-specific scheduling

Drawbacks:

- Blocking system calls can block entire process
- Errors propagate to whole process
- No clock interrupts, scheduler runs at same level as thread
- No benefits from multi-processor systems
#### Kernel Level Threads
Kernel has full knowledge of all threads and processes

- A kernel thread is created for each user thread
- Requires specific kernel support

Benefit:

- Kernel has full control of thread scheduling May give more time to processes with more threads
- Errors stop threads not processes
- Kernel handles blocking system calls

Drawbacks:

- Operations much slower than user level threads
- Significant overhead for kernel. It must manage and schedule processes and threads
### Pthreads
POSIX (IEEE 1003.1c) specification of threads – A standardisedAPI
### Remarks: Threading Issues
# Interprocess Communication
## Process Cooperation
Two or more processes sharing state
Behaviour depends on **execution order** of multiple executions
Concurrency

- Pass data between processes
- Synchronisation
- Multipule writers one reader
- Mutiple readers one writer

Each process has a separate address space
### Producer Consumer problem
### Fundamental Communication Methods

1. Shared memory access
   - For processes to share data they must be able to access the same memory addresses
2. Process controlled **shared memory**
   - Allocate a portion of memory which two or more processes can access
   - Kernel allows process to expose a portion of its address space
3. **Message passing **
   - Provide kernel functions to pass data
## Shared Memory
A portion of memory spanning one or more address spaces
Process A exposes a portion of memory

- Can be within its address space

Shared memory area is grafted onto the address space of process B
All methods for accessing the shared memory must be written by the application programmer
Benefits:

- **Speed**: No requirement for context switching to kernel for communication
- **Easy access**: Processes write to shared memory as normal,**no distinction** between shared memory and own memory

Drawbacks:

- **No protection**: Up to processes to write data in a controlled way and all processes using memory need to be trusted
- **Synchronisation**: Requires mutual exclusion and protected critical regions
- **Limited modularity:** Must have pointer or ID to shared memory before it can be grafted. All sharing processes must agree shared definitions
### Shared Memory usage
### POSIX Shared Memory
### File Sharing and Mapping
Share data via file: **for larger data sharing**

- Each process writes to a specified file on the filesystem
- Slower to access than memory if on disk
- File access synchronised by locking

Map a file, byte for byte to a portin of memory
## Message Passing
Kernel provides mechanisms for sharing data and synchronisation
Many different implementations possible

- Direct or indirect addressing
- Synchronous or asynchronous
- Buffering

Functions: send and recive
### Messages
Message can be anything, so long as it's mutually comprehensible
Fixed length messages

- Simple to implement, fixed set of buffers
- Inconvenient for large data, requires multiple messages
- Length of message important, must balance copying overhead with multiple message sending

Variable length messages

- More difficult to implement but much more convinient
### Direct Addressing
Messages are received from and sent to specific processes

- send and receive methods explicitly name receiving or sending process (P and Q).

Symmetric addressing:

- send(Q, message)---receive(P, message)

Asymmetric addressing

- send(Q, message)---receive(id, message)
### Indirect Addressing
Messages sent to and received from mailboxes or ports

- Each mailbox has unique ID and processes can communicate via a number of mailboxes
- Can be shared by more than two processes
- Mailbox: A;----send(A, message)---receive(A, message)

One-to-Many: similar to a broadcast
Many-to-One: Client-server relationship; Sender identified by message headers
Many-to-Many
### Synchronous vs Asynchronous Communication
#### Synchronous

- Blocking send and receive: Block until the corresponding call is made in the other process
- Tight synchronisation, confirmation of receipt, no buffering overhead
- Cannot continue if no messages or no receivers
#### Asynchronous

- Send continues after message, receive continues if no message available
- Message buffering required, increased concurrency, receivers can poll
#### Mixed

- Asynchronous send – blocking receive: receiver waits until there’s something to do, sender not affected by receiver
### Different Uses of Message Buffers
Messages buffered in a temporary queue

- **Zero capacity**: Maximum length of 0, no message stored. Sender blocks until receive called
- **Bounded**: Limited capacity. If full, sender blocks until space freed by receiver
- **Unbounded**: Unlimited capacity. Sender never blocks
### Lost Messages
How to handle lost messages

- Message sent but never received – receiver crash, stopped listening
- Waiting receiver never receives a message – sender crashed, stopped sending

Crashing processes: Kernel knows when a process crashes, can notify listening or sending processes through terminate signals or synthesised messages
Timeout: Asynchronous send, Add timeout parameter to receive. Program can recover from errorstate **Critical in distributed systems**
### Synchronisation using message passing
Set up mailbox
```
create_mailbox(mutex);
send(mutex, null-message);
```
Both processes run:
```
while(TRUE){
	receive(mutex, null-message);
  critcal section
  send(mutex, null-message);
}
```
# Concurrency & Synchronisation(1)
## Concurrency
### Concurrency Issues
Processes and threads allow pseudo or true parallelism
Single CPU: time-sharing
Multi-CPU: execution in parallel
#### Too much milk example
### Synchronisation
Need to control access to shared resources
**Ensure that proper sequencing between executions is maintained when dependencies exist**
Applies to both threads and cooperating processes
### Critical Section
Part of cooperating processes (or threads) where a shared resource is accessed and must be **executed atomically**
Solution requires:

- **Mutual exclusion**: No two processes or threads can be in their related critical sections at the same time 
- **Progress**: Progress cannot be delayed by another process or thread not in a critical section
- **Bounded Waiting**: No process or threads should wait forever to enter its critical section (or deadlock possible)
## Synchronisation Mechanisms
### Synchronisation using message passing
见上文
### Petersons Algorithm
Operation:

- Two threads of alternating execution, i = 0 and j = 1 
- **Turn**: Whose turn is it?
- **Flag**: Want to enter critical section
- Spinlock (**Busy Waiting in a loop**)
```
int turn;
bool flag[2] = 0;
do{
	flag[i] = TRUE;
  turn = j;
  while(flag[j] && turn == j);
  CRITICAL_SECTION
  flag[i] = FALSE;
  REMAINDER_SECTION
}while(TRUE);
```
### Hardware Solutions
Requires CPU instruction set support: Can disable interrupts, avoids pre-emption
Atomic Instructions:

- TSL (test-set-lock), may only lock a single processor
- SWAP, compare and swap registers atomically

Sleep and wakeup

- Avoids spin locking, but inadequate for mutual exclusion
### Semaphores
S as a shared variable and implemented as an integer

- Access through two atomic operations
- **Wait** locks entry to a critical region
- **Signal** releases the lock
- S initialised to** number of resources**
- **Busy Waiting** (spinlock) can yield instead of no-op
```
wait(S){
	while(S<=0);
  S--;
}
signal(S){
	S++;
}
```
Benefits

- Machine-independent
- Simple
- Powerful. Embody both exclusion and waiting
- Correctness is easy to determine
- Work with many processes
- Can have many different critical sections with different semaphores
- Can acquire many resources simultaneously
- Can permit multiple processes into the critical section at once, if that is desirable
### Mutex

- Binary Semaphore
- No counting
- The process or thread to lock the mutex must be the one to release it
### Monitors
Higher-level synchronisation tool (or **ADT**) which has:

- Atomic Operations
- Mutex (lock)
- Shared data
- Condition variable

Uses condition variable to signal other processes
Only one process can be active in a monitor at any instance
Incorporated into many languages
# Concurrency & Synchronisation(2)
## Deadlock
### Characteristics of Deadlock

- **Mutual Exclusion**: Only one process at a time can use a resource
- **Hold and wait**: A process holding at least one resource and waiting for another to be released
- **No Preemption**
- **Circular Wait**
### Dealing with Deadlocks

- Use a protocol to prevent or avoid deadlock
- Detect it and recover
- Ignore it (Most operating systems do this, left up to the programmer)
#### Prevention:

- 只读
- 请求前先释放
- No preemption
   - If a process is forced to wait, release all other held resources
   - If a resource is requested and blocked, check to see if any holders of the same resource are themselves blocked
- Circular Wait
   - Strict ordering of resources, process can only request Rj if it releases all Ri where i <j
#### Avoidance
Safe State

- Sequence <P1, P2, …, Pn> is safe if for each Pi, the resources that Pi can still request can be satisfied by currently available resources + resources held by all the Pj , with j < i

Resource Allocation Graphs: detect circles
Bankers Algorithm (Dijkstra)

- Specifies maximum needed resources, the process can never ask for more than this amount.
- Total resources given can never exceed resources available
- Conditions for allocation request
#### Deadlock Detection
### Deadlock Recovery
Very difficult, OS has only two options:

- Termination
- Preemption (OS **takes back** resources)
# Scheduling
## Introduction
### Type of scheduling
Long term: Chooses which jobs/tasks to **load into memory** (e.g., RAM) for CPU scheduling
Medium Term: Process scheduling: controls process **swapping in/out of** memory
Short Term: schedule the processes that are **already in memory** and **ready** to run
Normal Kernel: Balances processor time across processes using **priorities**
RTOS: Assigns processor time based on **deadlines** and **priorities**
### CPU-I/O Cycles

1. CPU bursts - performing calculations:
2. I/O bursts - transferring data
### The CPU Scheduler
CPU Scheduler - short-term scheduling
Choosing the next process to run

- FIFO
- SJF: Shortest Job First, runs the next shortest **CPU burst**
- Priority-based: highest priority runs first
- Round Robin: uses time quantums (slices) to fairly distribute CPU time
- Multilevel Queues: the hybrid approach

前三个可能导致有的进程永远无法被执行
RR 可能导致频繁的上下文切换
Scheduling Objectives: e.g., minimising idle time of the CPU, I/O waiting
### Terminology

- **CPU Utilization**: percentage of CPU cycles used for computation
- **Throughput**: the number of processes that are completed per time unit
- **Execution Time**: time required for the process to be executed by CPU
- **Waiting Time**: the time a process has been waiting in the ready queue
- **Turnaround/Makespan**: time from issuing the process to its completion
- **Response Time**: 1) the time from the submission of a request until the first response is produced; 2)* time from issuing the process to its completion **(* in this slides, referred as the turnaround time)**

To tackle **long-tail** problem, we typically optimize the minimum or maximum (or 95/99 percentile) values rather than the average, e.g., we may want to minimize the maximum response time **(min-max)**
### Conditions for Scheduling

- Process switches from running to waiting, e.g., I/O request
- Process switches from running to ready, e.g. timer interrupt
- Process switches from waiting to ready, e.g. I/O event occurs
- Process switches from running to terminated, e.g., termination

1,4: Switch process(non-preemtive)
2,3: Continue with the same process or **switch**(优先度考虑)
### Preemption
Processes are usually assigned with priorities. At times it is necessary to run a certain process that has a **higher priority** before another process although it is running. Therefore, the running process is **interrupted** for some time and **resumed** later when the priority process has finished its execution. This is called **preemptive scheduling**, e.g., Round robin
### Dispatching
The innermost porting of the OS
A dispatcher runs on each processor core
Allocates the CPU to the selected precess
The dispatcher is responsible for:

1. Scheduling and running processes
2. Switching context
3. Switching between user and system control 
#### Scheduling processes
L11,12 P11
#### Context switching
A **context switch** occurs when the CPU switches from one process to another.
## Algorithms
### Meterics
**Execution Time(C)**: Time for CPU to run the process
**Waiting Time**: excludes Request Time but includes I/O Wait and preempted, etc.
**Response Time** = Waiting Time + C
**Average Waiting Time (AWT)** = Σ Waiting Time/n
**Average Response Time (ART)** = Σ Response Time/n
### FIFO
L11,12 P16
### SJF
L11, 12 P17
Choose the next shortest CPU burst time
Can be:

- Non-preemptive
- Preemptive: Based on Shortest Remaining Time First - it occurs when a new process is in the ready queue with a shorter burst time than the remaining burst time of the currently executing process
### Priority Scheduling
Generalises SJF by assigning priorities

- SJF priorities are the inverse of the next expected burst time, i.e. shorter burst = higher priority
- The process with the highest priority gets scheduled first

Priorities are assigned using integers in a fixed ranges

- Linux nice value: -20 to 19, 19=lowest priority
- Windows: 0 to 31, 31=highest; Use priority classes (6) & priority levels (7) to give 42 base thread priorities

Can be preemptive or non-preemptive
Can result in indefinite blocking of processes

- Often solved using aging to increase priorities as they wait
### Round Robin
One of the most common schedulers used in OSs
Similar to FIFO but **assigns CPU burst limits using time quantum (aka. Time slices/quanta)**
Gives the appearance of sharing the CPU equally across all processes
The above Time Quantum is not optimal

- Too small results in too many context switches
- Too big results in FIFO scheduling

L11,12 P20-21
time quantum 和 burst 任意一个达到了都要进行一次切换，其中 time quantum 是全局的，burst 是单次的
### Multi-Level Queues
Each class of priority is given:

- Separate Queues
- Appropriate scheduling algorithm

The highest levels of priorities can indefinitely block lower priority tasks
The dispatcher can also split the CPU time across the priority levels
There are various methods for scheduling different priorities:

- **Time slicing** (previous slide), each queue gets a percentage of CPUtime
- **Fixed Priority**, clear all higher priority queues first (can result in indefinite blocking of lowerpriorities)
- **Multivelevel feedback queue**, processes get demoted down the priority levels if they don't finish in their assigned time quantum, but they can also be upgraded when required (aging)
## Processors
### Muti-Processor Scheduling
Objective: effectively utilise and balance all CPUs at all times
Asymmetric multiprocessing: one processor acts as the master and runs kernel code, the other runs user code
Symmetric multiprocessing (SMP): each processor schedules processes from a ready queue
Processor affinity: attempting to keep a process running on a single processor to utilise the benefit of caching
Multithreaded systems can support 2+ processes executing “concurrently” on a core

- Uses hardware scheduling, often Round Robin
- Switches on memory access, this is not I/Owaiting
#### Max-min Fairness Strategy

- Maximize the minimum allocation received by a user/process (i.e., it tries to satisfy user with the lowest demand first)
- Simply deriving from game theory
- Implemented by modified round-robin, proportional resource sharing, weighted fair queuing
### More Complicated Scheduling
DRF (Dominant Resource Fairness)
Distributed Resource Scheduling
# Virtual Memory
## Memory Fragmentation
### Internal/ External Fragmentation
### Paging
Is another solution to external fragmentation
What: paging is a memory management scheme that permits the **physical address space** to **be non-contiguous** (e.g. several holes may now be allocated to a process in its **“virtual” address space** in the form of “**contiguous pages**”, making the memory less fragmented)
Paging is typically implemented by closely** integrating the hardware and the OS**
Every address generated by the CPU is divided into two parts: 1) the **page number**, and 2) the **offset** (or the relative address)
## Virtual Memory
Virtual memory is a memory management technique that maps (**virtual**) memory addresses used by a program into **physical** addresses in memory
A process or task sees the main memory as a contiguous address space
The OS manages virtual address spaces and the assignment of real memory to virtual memory (The **MMU** automatically translates virtual addresses to physical addresses)
### Benefits of Virtual Memory

- Freeing applications from managing a shared memory space
- No longer constrained by limits of physical memory
- Each program takes less memory while running
- More programs run at the same time
- More programs running = Better CPU utilization 
- Less I/O needed to load or swap programs into memory
- Increased **security** due to memory isolation (but how?)
## Paging
### Frames and Pages
Paging partitions memory into small equal fixed-size chunks, called Frames

- frame 是基于物理内存的区域划分

Frames map to **physical memory**
It divides each process into the same size chunks, call Pages

- page 是基于 process 内存的区域划分

Pages do not need to be contiguous in physical memory. Pages form **logical (or virtual) memory**
Some OSes have multiple distinct sizes for pages (e.g. Solaris has 8kb and 4MB pages)
### Free frames
An OS needs to record what frames are free and what are taken
A **Frame Table** stores this list: 

- Is a frame free or used?
- If it is used, which process is using it?

A **Page Table** maps between a virtual and physical address
### Structure of Page Table

- **Straight-forward methods**:  structures for paging can get huge
- **Hierarchical Paging** (multi-level page tables)
- **Hashed Page Tables**
- **Inverted Page Tables** – one page table for all processes (cheaper but slower to search)
### Hardware Implementation of Page Table
**The page table is kept in main memory**
**Page-table base register (PTBR)** points to the page table
**Page-table length register (PTLR)** indicates size of the page table
Every data/instruction access requires two memory accesses:

- Page table
- The data/instructions

**Translation Look-aside Buffers(TLBs):**

- A special fast-lookup hardware cache
- TLBs typically small (64 to 1024 entries)
### Detecting if a page is in memory
Each page table entry has a valid–invalid bit

- v - if in-memory
- i - if not-in-memory

Initially the valid-invalid bit is set to i on all entries
During MMU address translation, if a valid-invalid bit in a page entry is i, then a **page fault** is generated
### Page Faults
**Handling Page Faults:**

1. The OS needs to decide:
   - If reference is wrong then abort
   - Just not in memory
2. Find free frame
3. Swap page into frame via scheduled disk operation
4. Reset tables to indicate page now in memory
5. Restart the instruction that caused the page fault
### Demand Paging
Paging System with Swapping (**which is costly, don’t do this!**) 
Bring a page into memory **only when it is needed**

- Less I/O needed
- Less memory needed
- Faster response
- More users

Demand paging stages:

- L13 P20
### Process Isolation
Process isolation is a set of different hardware mechanisms (e.g. lookup cache) and software (e.g. exception handlers) technologies designed to protect each process from other processes on the operating system. It does so by preventing process A from accessing to process B
### Page Tables and Security
A page table may contain other bits apart from the valid-invalid bit, including page access control (rwx) and process IDs. The paging hardware can therefore prevent reading/writing/executing memory the process does not have access to.
The effect of attempting to access memory not mapped by the paging hardware will lead to an **exception** and the Operating System will have to handle it by calling the corresponding **exception handler**.
## Page Replacement
Find the location of the desired page on disk
Find a free frame

- If there is a free frame, use it
- If there is no free frame, select a Victim Frame. Write victim frame to disk if dirty (or modified)

Bring the desired page into the (newly) free frame; update the page and frame tables
Continue the process by restarting the instruction that caused the trap
Note: now **potentially 2 page transfers for a page fault**
### Page Replacement Algorithms
需求:

- Want lowest page-fault rate on both first access and re-access
- Repeated access to the same page does not cause a page fault
#### FIFO
#### LRU (Least Recent Used)
判断最不常用的实现:

- Counter implementation
   - Every page entry has a counter; every time page is referenced through this entry, copy the clock into the counter
   - When a page needs to be changed, look at the counters to find smallest value
   - Search through table needed
- Stack implementation
   - Keep stack of page numbers in double link form
   - Page referenced:
      - move it to the top
      - requires 6 pointers to be changed
   - each update more expensive
   - No search for replacement 
### Simple Sementation
Unlike paging, **segmentation **uses multiple separate address spaces for various segments of a program
Each segment may have different lengths and could change during execution
When a process is loaded into memory, its different segments can be located anywhere
Each segment is fully packed with instructions and data **without internal fragmentation**
There is external fragmentation; can be reduced when using small segments
### Segmentation vs Paging
Segmentation requires more complicated **hardware** for address translation
Segmentation is **visible** to the programmer whereas paging is **transparent**
Segmentation suffers from external fragmentation while paging only generates small internal fragmentation
However, segmentation allows procedures and data be **distinguished and separately protected**
Segmentation also helps sharing between several processes
# Virtualization
## What is virtualization
Virtualization can be broadly defined as the (beneficial) separation of a conceptual resource or service from the physical means of providing it
### Important Definitions
Virtual Machine

- A representation of a real machine
- Using software to provide (emulate) the operating environment

Guest Operating System

- An operating system running inside a VM

Host Operating System

- The operating system that the VM runs upon

Virtual Machine Monitor (VMM) / Hypervisor

- The middleware that virtualizes the resources
- Sits between the underlying resources and virtual machines
### Benefits and Drawbacks
Benefits:

- Freedom of choice for operating system (i.e. testing environment)
- Consolidates server and infrastructure
- It saves time and money
- Makes it easier to manage and secure desktop environments
   - Isolates “unsafe” programs into VM
   - Can copy VMs from one machine to another
   - Can setup copies of the same OS/Software setup quickly

Drawbacks:

- Guest OS hardware requirements, higher!
   - Host needs more RAM, disk space, etc.
- Can require training to operate
- Cannot be 100% accurate (i.e. close enough)
- Hardware issues, e.g. Intel IA-32
### VMM: Virtual Machien Monitor
Qemu, VMware Player, Microsoft Virtual PC etc
Sometimes called Emulation, e.g. x86 Emulation
### Hypervisor Classification
Type1 - Hypervisor

- Sits on the bare metal computer
- All the guest operating systems are a layer above the hypervisor
- Hypervisor is the first layer over the hardware
- Example: Microsoft Hyper-V

Type2 - Hosted Hypervisor

- Run over a host operating system
- Is the second layer over the hardware
- Guest operating systems run a layer over the hypervisor
- Example: FreeBSD
### Hypervisor Properties

1. **Equivalence**: Programs should behave identically to direct execution on equivalent hardware
2. **Resource Control:** Hypervisor must have complete control over virtual resources 
3. **Efficiency**: A significant proportion of machine instructions must be executed without interference by the Hypervisor
## Virtualization Techniques
### Full Virtualization
In **Full Virtualization** the guest OS is **unaware** that it is in a virtualized environment, and it thinks that it talks to the **“hardware”** directly

- **Host OS** simulates enough hardware to allow an **unmodified guest OS**
- Potentially **different architectures** can run in isolation
- Considerable **performance penalty** due to the overhead associated with emulating hardware at the transistor level
### Hardware Assisted Virtualization
**Hardware Assisted Virtualization** is a **type of Full Virtualization** where the microprocessor architecture has special instructions to aid the virtualization of hardware (**rather than fully depending on the host OS to simulate the hardware devices**). It utilises hardware capabilities, currently in the form of Virtual Machine Extensions (VMX), e.g. Intel VT & AMD V
**Improving performance of a virtual machine**

- Instructions directly passed to the host processor without having to be interpreted and isolated
- limits guest OS to same instruction set as the host machine

**Complete Hardware Assisted Virtualization** of all computer subsystems such as I/O and memory management, has yet to be implemented completely in any VMM
### Partial Virtualization
Simulation of most but not all the underlying hardware of a host
Supports resource sharing but does not completely guarantee isolated guest OS instances
This approach is utilised in:

- Para-Virtualization
- Hybrid Virtualization
- Operating System-Level Virtualization
### Para-Virtualization
In Para-Virtualization the **guest OS is aware that it is a guest, and it talks to the host OS directly** (rather than to the simulated hardware)

- Guest OS has to be modified
- Provides specially defined ‘hooks’ to do some tasks in the host and guest OS’s
- The VMM in a para-virtualized platform is simpler because the critical tasks are now performed in the OS rather than by theVMM
   - Since the virtualization overhead decreases the **performance increases**

Disadvantages:

- Reduced compatibility & portability because of the modified OS
- High cost of maintenance because of the deep OS modifications

Example: KVM
### Full Virtualization vs Para-Virualization
Full Virtualization:

- Does not need to modify guest OS
   - Critical instructions are emulated by software through the use of **binary translation**
   - This approach of binary translation slows down the performance
   - e.g. VMware

Para Virtualization

- Needs to modify guest OS
   - Non-virtualizable instructions are replaced by hypercalls that communicate directly with the hypervisor or VMM
   - Reduces the overhead
   - Cost of maintaining paravirtualized OS is high
   - Supported by Xen, VMware ESX
### Hybrid Virtualization
Combines:

- Hardware Assisted Virtualization
- Para-Virtualization

Obtains near native performance from a guest OS
Disadvantages of both
Provides an excellent foundation for the creation of new **Cloud-based systems**, reducing the number of physical machines needed at peak demand and thus hardware running and setup costs
Most VMMs support multiple types of virtualization so these 
disadvantages can be somewhat mitigated
### Operationg System-Level Virtualization
Isolating multiple user space instances
Guest OS must be same as host
Guest executes at native performance
## Case Study
### The XEN Architecture
### VMware ESX Server for Para-Virualization
### KVM
### VirtualBox
### Major VMM and Hypervisor Providers
### Cloud Infrastructure Managers
# RAID and Distributed File Systems
## RAID
**Redundant Array of Independent (Inexpensive) Disks**
Developed to avoid the “pending I/O” crisis:

- Attempts to improve disk performance and/or reliability 
- Multiple disks in an array can be accessed simultaneously
- Performs accesses in parallel to increase throughput
- Additional drives can be used to improve data integrity
### RAID Principles
**Strip**: Data store on one disk in a continuoussection
**Stripe**: Set of strips at the same location across multiple disks
Fine-grained Strip (Bit/Byte Level)

- High transfer rates (many disks service request at once)
- Array can only process one request at a time

Coarse-grained strip (block level)

- Might fit entire file on one disk
- Allow multiple requests to be filled at once

Large number of disks **decreases the mean-time-to-failure** (MTTF)

- More disks provide more points of failure
- Data stored in many RAID levels can be lost if more than one drive in array fails

Disk mirroring

- One disk is simply a copy of another
- Simple way to achieve redundancy and fault tolerance, but incurs significant storage overhead

RAID Controller – Special hardware dedicated to RAID operations

- Offloads most responsibility from operating system/processor
- Can be expensive
### RAID 0
**Block-level stripping** over several disks (i.e.** a data block** stripped into bytes).
Performance advantage, as it is possible to read a file in parallel.
No data protection - less reliable than a single disk, as all data is lost if single disk in the array stripe fails.
### RAID 1
Data mirroring.
Two copies of the data are held on two physical disks, and the data is always identical. 
Has a performance advantage, as reads can come from either disk, and is simple to implement.
However, expensive in large disk subsystems, as twice as many disks are needed to store data.
### RAID 2
RAID 2 stripes data at **bit level **(rather than at block level in RAID 0) and uses a Hamming code for error correction.
Disks are synchronized by the controller to reach the index at the same time, so generally cannot service multiple requests simultaneously. Extremely high data transfer rates are possible.
All hard disks now implement error correction. This makes RAID 2 error correction redundant and unnecessarily complex.
**There are no commercial applications of RAID 2.**
### RAID 3
**Blocks of data are stripped** into bytes and over an array of disks, then **parity data** (for improved error detection) is written to a dedicated parity disk (**RAID 0 + parity disk**).
Successful implementations usually require that all the disks have synchronised rotation.
Very effective for large sequential data, such as satellite imagery and video.
### RAID 4
Data is written in blocks onto the data disks (i.e. **not stripped**).
Parity is generated and written to a dedicated parity disk
### RAID 5
Data is written in blocks onto data disks, and parity is generated and **rotated** around the data disks. 
Good general performance (unless software implemented), and reasonably cheap to implement.
Used extensively for general data.
### RAID 6
RAID 6 extends RAID 5 by adding another parity block.
It uses block-level stripping with two parity blocks distributed across all member disks.
Does not have a performance penalty for read operations, but does for write operations.
## Distributed File Systems
### Networks and File systems
Data becomes big and is everywhere
Must be accessible across a network (LAN, WAN)
Performance constraints:

- Network Speed vs Direct Access
- Network Availability (or connectivity)

Other issues:

- **Virtual**: Transparency of access (local and remote)
- Integrating a DFS seamlessly into local file systems
- Security (e.g. **access control**)
### Google File System
**Key Architectural Assumptions:**

- Commodity hardware (cheap but generally scalable)
- High component failure rates
- “Modest” number of huge files
- Multi-gigabyte files are common
- Files are write-once ones, mostly appended to
- Perhaps concurrently

Design Decisions:

- Files stored as chunks
   - Fixed size (64MB)
- Reliability through replication
   - Each chunk is replicated across 3+ chunkservers
- Single master to coordinate access
   - Simple centralized management
- No data caching - little benefit due to large datasets
- Simple APIs
### Hadoop Distributed FIle System
Apache Hadoop’s HDFS, inspired by the Google File System
Goals:

- Store large data sets
- Cope with hardware failures
- Emphasize streaming data access

Design To:

- Reliably store very large files across machines in a large server cluster
- Files are "write once" and have strictly one writer at any time

Each file as a sequence of data blocks:

- All are the same size except the last block
- Replicated for fault tolerance
- Block size and replication factor are configurable per file
- Typical size is 64 MB
- Typically, 3 replicas of a block
### HDFS Architecture
**HDFS follows a master/slave architecture**
consists of:

- NameNode: A master server that manages the filesystem namespace and regulates access to files by clients - One per file system
- DataNodes: - One per node in the cluster
   - Serve Read and Write requests from clients
   - Perform block creation, deletion, and replication
### Distributed File Systems...
Don't move data to workers… move workers to the data!
Why:

- Not enough RAM to hold all the data in memory
- Disk access is slow, but disk throughput is reasonable
# Computing at Scale
## Operational Data
## Latency: the long-tail problem
When a job can be partitioned into many tasks which may be executed in parallel on different servers in a Cloud cluster, the execution of the job should be sped up (at least in theory).
However, the job is often slowed down due to the **long-tail problem**, i.e., a small number of tasks (servers) make no or very slow progress, and the job has to wait for the completion of the slowest task. 
### Methods for Mitigating Long-Tails
An application job consists of many tasks which may be executed in parallel over a large number of servers

1. **Task Cloning**: For each task, execute many copies of it over different servers, and use the results from the quickest server, thereby reducing the tail of the job execution – **but costly**
2. **Kill and Re-try**: When the system detects that the execution of a task is too slow, it kills the task and re-starts it again, possibly on a different server in the hope that the re-try will be quicker – **but based on a sort of luck**
3. **Speculation**: When the system detects that the execution of a task is too slow, it starts the same task on another server, and uses the results from the quicker server, thereby possibly reducing the tail – **but not always work**
4.  An AI-based method
# Security
## Threats and Attacks
### THe Security Problem
Intruders (crackers) attempt to breach security
**Threat** is potential security violation
**Attack** is the attempt to breach security
Attack can be accidental or **malicious (or intentional)**
Easier to protect against accidental than malicious misuse
### Computer Security
Computer security, also known as cyber security or IT security, is the protection of computer systems from the theft and damage to their hardware, software or information, as well as from disruption or misdirection of the services they provide
### Security Violation categories
Breach of confidentiality

- Unauthorized reading of data

Breach of integrity

- Unauthorized modification of data

Breach of availability

- Unauthorized destruction of data or services

Theft of service

- Unauthorized use of resources

Denial of service (DOS)

- Prevention of legitimate use
### Security Violation Methods
Masquerading (breach **authentication**)

- Pretending to be an authorized user to escalate privileges

Replay attack

- As is or with **message modification**

Man-in-the-middle attack

- Intruder sits in data flow, masquerading as sender to receiver and vice versa

Session hijacking

- Intercept an already-established **security session** to bypass authentication
### Security Measure Levels
Impossible to have absolute security, but could make cost of attacks sufficiently high to deter most attackers
Security must occur at four levels to be effective:

- Physical (Hardware)
   - Data centers, servers, connected terminals
- Human
   - Avoid social engineering, phishing etc
- Operating System
   - Protection mechanisms (e.g. access control), debugging
- Network
   - Intercepted communications, interruption, DOS
## Fundamentals
## Security in Computing Systems
Typically in computing systems, we need to implement some form of security. This primarily comes down to ensuring correct **access control** and **secure communication. **
Access control ensures that only an entity that is **authorised** to access, modify, or use a service/piece of information can do so
This becomes even more important in distributed, networked systems, such as **Clouds** and Web-based Software-as-a-Service (SaaS)
The **communication medium** is insecure - information in transit may be intercepted, read, or altered by unauthorised parties.
Access control is centered around two important concepts:

- Authentication
- Authorisation

End-to-end consistency of security mechanisms is required.
While the source and destination systems for information transfer may be as secure as required, information may pass through intermediate systems; their degree of security must be specified (e.g. who is the weakest link?). 
## Authentication
The idea of authentication is to establish the identity of a principal involved in any computational procedure.
A** principal** can be thought of as a process running a program on behalf of a logged-on user. A principal can also be a person, a program, a machine, or a device etc.
### Authorisation and Validation
An **authorisation** policy specifies which principals may be allowed to access a service and in what way. The system must have mechanisms which can enforce the policies, i.e. **policy enforcement**
When you download and run a program, that program runs** with all your access rights**. It could read, overwrite or delete your files. It is therefore desirable that the source of software can be authenticated and verified (e.g. from a trusted site)!
### Authentication in Distributed Systems
Authentication in distributed systems is more complex than in centralised systems because secret information, such as password, needs to be transferred **across a network**.
The “**pair of keys**” method: A principal may own encryption keys for use in transforming data to and from an encrypted representation. A principal may use a **private key**, known only to itself, to encrypt the data. The recipient of such data can be sure that the data originated from the expected principal because that principal’s publicly available key (i.e. **public key**) transforms the data back to an understandable form
Encryption is typically based on some assumptions of computational complexity (e.g. your 4-digit PIN).
### Asymmetric Encryption for Secure Communication
**Asymmetric encryption** based on each user having two types of keys:

- **public key** - published key used to encrypt data
- **private key** – key known only to individual user used to decrypt data

Must be an encryption scheme that can be made public without making it easy to figure out the decryption scheme (**as long as the private key is secure and safe**) 

- Most common is **RSA** encryption algorithm
- Efficient algorithm for testing whether or not a number is prime
- No efficient algorithm is known for finding the prime factors of a number (as long as the key is long enough)
### Asynchronous encryption and PKI
A technology in modern distributed systems that helps to support multi-party authentication is **PKI** – Public Key Infrastructure.
PKI is an example of asynchronous encryption using **public/private key pairs**, and is a crucial concept
At the most basic level, documents encrypted with a private key can be decrypted using a public key, and **vice-versa**.
By sharing your public key with the world, anyone can verify that you are who you say you are by decrypting your messages with that key
In practice, encrypting and decrypting entire messages would be an un-scalable practice in large-scale distributed systems.Instead, we typically create a hash of the message (which may also include other metadata) and encrypt that using our private key. 
This encrypted hash is known as a **digital signature**. 
The combined infrastructure that is required to register components, create their public/private keys, and verify signatures is typically known as **Public Key Infrastructure (PKI)**
We still need to deal with **authorisation.**
## Digital Signature
Typically, a file system can compare two files using their hashes; when identical hashes are found, the two files are regarded as **identical**
### Secure Hash Algorithms
The **SHA** (Secure Hash Algorithm) is one of a number of cryptographic hash functions
**SHA-1** is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST. It takes a block of information and produces a short **40-character hash**
**SHA-256** algorithm generates an almost-unique, fixed size **256-bit (32-byte)** hash. Hash is a one way function – it cannot be decrypted back
**Example: L17,18 P22**
### Hash Functions
Each file is assigned an identifier (i.e. a hash value), calculated by the software system, typically using hash functions
In many implementations, the assumption is made that if the identifier is identical, the data is identical, even though this cannot be true in all cases due to the **pigeonhole principle**
### Hash Collision
If two different pieces of information generate the same hash value, this is known as a **collision**
The probability of a collision depends upon the hash function used, and although the probabilities are small, they are always non zero
A system may offer **bit-for-bit validation** of original data for guaranteed data integrity
Sha-1, Sha-256 etc. provide a far lower probability of data loss than the risk of an undetected and uncorrected hardware error in most cases and can be in the order of 10−49% per petabyte (1,000 terabyte) of data
## Protection
### Principles of Protection
Guiding principle – **principle of least privilege**

- Programs, users and systems should be given just enough **privileges** to perform their tasks
- Limits damage if entity has a bug, gets abused
- Can be static (during life of system, during life of process)
- Or dynamic (changed by process as needed) – **domain switching, privilege escalation** 
- “Need to know” – a similar concept regarding access to data
### Domain Structure
**Domain** can be user, process, procedure etc.
Access-right = <object-name, rights-set> where rights-set is a subset of all valid operations that can be performed on the object
Domain = set of access-rights
### Access Matrix
View protection as a matrix (access matrix) 
Rows represent domains
Columns represent objects
**Access(i,j) **is the set of operations that a process executing in Domaini can invoke on Objectj
**Example: L17,18 P29**
### Use of Access Matrix
If a process in Domain Di tries to do “op” on object Oj , then “op” must be in the access matrix
User who creates object can define access column for that object
Can be expanded to dynamic protection
Access matrix design separates mechanism from policy:

- Mechanism
   - Operating system provides access-matrix + rules
   - It ensures that the matrix is only manipulated by authorized agents and that rules are strictly enforced
- Policy
   - User dictates policy
   - Who can access what object and in what mode

But it does not solve the general damage confinement problem
# Others
## Mobile OSs
## Real-Time OSs
## HPC and Cloud OSs
